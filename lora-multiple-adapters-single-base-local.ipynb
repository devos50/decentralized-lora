{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9c8ff0-6653-4de1-8bc2-dd2351e99863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martijndevos/miniconda3/envs/delora/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AutoModelForSequenceClassification, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForLanguageModeling, ViTImageProcessor, ViTForImageClassification\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "from datasets import load_dataset\n",
    "from torch.nn.functional import cross_entropy  # Assuming classification task\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d22850-6659-45cc-b95b-fac49d8b6e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# General configuration\n",
    "USERS = 10\n",
    "ROUNDS = 500\n",
    "LOCAL_STEPS = 5\n",
    "EVAL_INTERVAL = 10\n",
    "LORA_RANK = 8\n",
    "ONLY_MERGE_ADAPTERS = False  # Whether we only merge LoRA adapters or all trainable parameters\n",
    "\n",
    "# text classification:  \"SetFit/20_newsgroups\", \"imdb\", \"ag_news\", \"emotion\" or \"yelp_review_full\"\n",
    "# image classification: \"uoft-cs/cifar10\"\n",
    "# next-word prediction: \"wikitext\" (+\"wikitext-2-raw-v1\" as config)\n",
    "# summarization:        \"xsum\"\n",
    "DATASET = \"ag_news\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "TASK = \"txt_classification\"  # \"img_classification\", \"txt_classification\", \"prediction\", \"summarization\"\n",
    "DATASET_DISTRIBUTION = \"uniform\"  # \"uniform\" or \"dirichlet\"\n",
    "ALPHA = 0.1\n",
    "FT_ALGORITHM = \"lora\"  # \"lora\" or \"head\" (the latter just fine-tunes the classification head)\n",
    "\n",
    "# text classification:   \"roberta-base\", \"distilbert-base-uncased\"\n",
    "# image classification:  \"google/vit-base-patch16-224\"\n",
    "# next-token prediction: \"gpt2\", \"distilgpt2\"\n",
    "# summarization:         \"t5-small\"\n",
    "BASE_MODEL = 'roberta-base'\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device: %s\" % DEVICE)\n",
    "\n",
    "if FT_ALGORITHM == \"head\" and USERS > 1:\n",
    "    raise RuntimeError(\"Head-only FT can only be done with a single user for now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70ea9b5-4974-475b-8dc5-8a2e586ba801",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK in [\"txt_classification\", \"img_classification\", \"summarization\"]:\n",
    "    dataset = load_dataset(DATASET, cache_dir=\"datasets\")\n",
    "elif TASK == \"prediction\":\n",
    "    dataset = load_dataset(DATASET, DATASET_CONFIG, cache_dir=\"datasets\")\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown task %s\" % TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e9f21c-9c66-47f5-877c-035dbd4dc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"SetFit/20_newsgroups\":\n",
    "    # We need to do some small transformations\n",
    "    unique_classes = sorted(set(dataset['train']['label']))\n",
    "    label_feature = ClassLabel(names=unique_classes)\n",
    "    dataset = dataset.cast_column('label', label_feature)\n",
    "    dataset = dataset.remove_columns('label_text')\n",
    "elif DATASET == \"uoft-cs/cifar10\":\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "    def transform(examples):\n",
    "        inputs = feature_extractor(examples['img'], return_tensors='pt')\n",
    "        inputs['labels'] = torch.tensor(examples['label'])\n",
    "        return inputs\n",
    "    \n",
    "    dataset = dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a70e18-828f-48de-87d9-dc738ff8cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e33921-3e64-465c-913b-de1f1aa20b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"txt_classification\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    \n",
    "    def preprocess(examples):\n",
    "        tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "        return tokenized\n",
    "    \n",
    "    processed_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "elif TASK == \"prediction\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    \n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"])\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "    \n",
    "    block_size = 128\n",
    "    \n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    processed_dataset = tokenized_dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "    )\n",
    "elif TASK == \"summarization\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "\n",
    "    max_input_length = 1024\n",
    "    max_target_length = 128\n",
    "    if BASE_MODEL in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "        prefix = \"summarize: \"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "    \n",
    "    def preprocess(examples):\n",
    "        inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "        # Setup the tokenizer for targets\n",
    "        labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    processed_dataset = dataset.map(preprocess, num_proc=8, batched=True, remove_columns=[\"document\", \"summary\", \"id\"])\n",
    "else:\n",
    "    processed_dataset = dataset\n",
    "\n",
    "train_dataset = processed_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b93183-0f3d-4eb3-8301-246821f51f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_uniform(dataset, n_users):\n",
    "    # Shuffle the dataset to ensure randomness\n",
    "    shuffled_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    # Calculate the number of samples per user\n",
    "    num_samples = len(shuffled_dataset) // n_users\n",
    "\n",
    "    # Create a dictionary to hold the split datasets\n",
    "    split_datasets = []\n",
    "\n",
    "    for i in range(n_users):\n",
    "        start_idx = i * num_samples\n",
    "        end_idx = start_idx + num_samples if i < n_users - 1 else len(shuffled_dataset)\n",
    "\n",
    "        # Create a subset for the current user\n",
    "        user_subset = shuffled_dataset.select(range(start_idx, end_idx))\n",
    "        split_datasets.append(user_subset)\n",
    "\n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "def split_dataset_dirichlet(dataset, n_users, alpha):\n",
    "    # Get the number of classes\n",
    "    labels = dataset['label']\n",
    "    num_classes = len(set(labels))\n",
    "    \n",
    "    # Initialize a list to hold indices for each user\n",
    "    user_indices = [[] for _ in range(n_users)]\n",
    "    \n",
    "    # Seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate the Dirichlet distribution for each class\n",
    "    for cls in range(num_classes):\n",
    "        # Get indices for all samples of this class\n",
    "        cls_indices = np.where(np.array(labels) == cls)[0]\n",
    "        \n",
    "        # Get the number of samples for this class\n",
    "        np.random.shuffle(cls_indices)\n",
    "        num_samples = len(cls_indices)\n",
    "        \n",
    "        # Split the samples according to the Dirichlet distribution\n",
    "        proportions = np.random.dirichlet([alpha] * n_users)\n",
    "        \n",
    "        # Ensure the proportions sum to 1\n",
    "        proportions = proportions / proportions.sum()\n",
    "        \n",
    "        # Assign samples to each user based on the proportions\n",
    "        split = (np.cumsum(proportions) * num_samples).astype(int)[:-1]\n",
    "        cls_indices_split = np.split(cls_indices, split)\n",
    "        \n",
    "        for user, indices in enumerate(cls_indices_split):\n",
    "            user_indices[user].extend(indices)\n",
    "    \n",
    "    # Create datasets for each user\n",
    "    split_datasets = []\n",
    "    for indices in user_indices:\n",
    "        split_datasets.append(dataset.select(indices))\n",
    "    \n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "if DATASET_DISTRIBUTION == \"uniform\":\n",
    "    split_datasets = split_dataset_uniform(train_dataset, USERS)\n",
    "elif DATASET_DISTRIBUTION == \"dirichlet\":\n",
    "    split_datasets = split_dataset_dirichlet(train_dataset, USERS, ALPHA)\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown dataset distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72d07bc-ad01-4a40-b6c3-fd2ec38ee49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 12000 samples\n",
      "1: 12000 samples\n",
      "2: 12000 samples\n",
      "3: 12000 samples\n",
      "4: 12000 samples\n",
      "5: 12000 samples\n",
      "6: 12000 samples\n",
      "7: 12000 samples\n",
      "8: 12000 samples\n",
      "9: 12000 samples\n",
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 12000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(split_datasets):\n",
    "    print(f'{idx}: {len(data)} samples')\n",
    "\n",
    "print(split_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16fce8e2-ebd5-4017-9b6f-ce065dc67bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model():\n",
    "    if TASK == \"txt_classification\":\n",
    "        # Extract the number of classess and their names\n",
    "        num_labels = dataset['train'].features['label'].num_classes\n",
    "        class_names = dataset[\"train\"].features[\"label\"].names\n",
    "        print(f\"number of labels: {num_labels}\")\n",
    "        print(f\"the labels: {class_names}\")\n",
    "        \n",
    "        # Create an id2label mapping\n",
    "        # We will need this for our classifier.\n",
    "        id2label = {i: label for i, label in enumerate(class_names)}\n",
    "        \n",
    "        pretrained_model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, id2label=id2label, cache_dir=\"models\")\n",
    "    elif TASK == \"prediction\":\n",
    "        pretrained_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, cache_dir=\"models\")\n",
    "    elif TASK == \"img_classification\":\n",
    "        pretrained_model = ViTForImageClassification.from_pretrained(BASE_MODEL, num_labels=10, ignore_mismatched_sizes=True, cache_dir=\"models\")\n",
    "    elif TASK == \"summarization\":\n",
    "        pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, cache_dir=\"models\")\n",
    "    return pretrained_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaf4e513-35a3-4956-b607-4c82132b17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset=processed_dataset['test'].shard(num_shards=2, index=0)\n",
    "test_dataset=processed_dataset['test'].shard(num_shards=2, index=1)\n",
    "\n",
    "if TASK == \"txt_classification\":\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "elif TASK == \"prediction\":\n",
    "    def text_collate(examples):\n",
    "        input_ids = torch.stack([torch.tensor(d[\"input_ids\"]) for d in examples])\n",
    "        labels = torch.stack([torch.tensor(d[\"labels\"]) for d in examples])\n",
    "        attention_mask = torch.stack([torch.tensor(d[\"attention_mask\"]) for d in examples])\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "    data_collator = text_collate\n",
    "elif TASK == \"img_classification\":\n",
    "    data_collator = None  # Use the default one\n",
    "elif TASK == \"summarization\":\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown task %s\" % TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b45d0bf0-c726-4fa9-bd92-69bd2dd7fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Adding LoRA adapter client_0\n",
      "Adding LoRA adapter client_1\n",
      "Adding LoRA adapter client_2\n",
      "Adding LoRA adapter client_3\n",
      "Adding LoRA adapter client_4\n",
      "Adding LoRA adapter client_5\n",
      "Adding LoRA adapter client_6\n",
      "Adding LoRA adapter client_7\n",
      "Adding LoRA adapter client_8\n",
      "Adding LoRA adapter client_9\n",
      "Adding LoRA adapter global\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Trainable parameter: {name}, shape: {param.shape}\")\n",
    "            trainable_params += param.numel()\n",
    "        total_params += param.numel()\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "if TASK == \"txt_classification\":\n",
    "    target_modules = None\n",
    "    if BASE_MODEL == \"distilbert-base-uncased\":\n",
    "        target_modules = {\"q_lin\", \"v_lin\"}\n",
    "    peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=LORA_RANK, lora_alpha=16, lora_dropout=0.1, target_modules=target_modules)\n",
    "elif TASK == \"img_classification\":\n",
    "    peft_config = LoraConfig(inference_mode=False, r=LORA_RANK, lora_alpha=16, lora_dropout=0.1, target_modules=[\"attention.query\", \"attention.key\"])\n",
    "elif TASK == \"prediction\":\n",
    "    peft_config = LoraConfig(task_type=\"CAUSAL_LM\", inference_mode=False, r=LORA_RANK, lora_alpha=16, lora_dropout=0.1)\n",
    "elif TASK == \"summarization\":\n",
    "    peft_config = LoraConfig(task_type=\"SEQ_2_SEQ_LM\", inference_mode=False, r=LORA_RANK, lora_alpha=16, lora_dropout=0.1)\n",
    "\n",
    "if FT_ALGORITHM == \"lora\":\n",
    "    base_model = get_pretrained_model()\n",
    "    peft_model = get_peft_model(base_model, peft_config).to(DEVICE)\n",
    "\n",
    "    # Create adapters for each user\n",
    "    for adapter_name in [\"client_%d\" % i for i in range(USERS)]:\n",
    "        if adapter_name not in peft_model.peft_config:\n",
    "            peft_model.add_adapter(adapter_name, peft_config)\n",
    "            print(\"Adding LoRA adapter %s\" % adapter_name)\n",
    "        peft_model.set_adapter(adapter_name)\n",
    "\n",
    "    # Create a global adapter\n",
    "    if \"global\" not in peft_model.peft_config:\n",
    "        peft_model.add_adapter(\"global\", peft_config)\n",
    "        print(\"Adding LoRA adapter global\")\n",
    "elif FT_ALGORITHM == \"head\":\n",
    "    # TODO fix this\n",
    "    pass\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown FT algorithm %s\" % FT_ALGORITHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "624aa734-a5a9-4135-b305-0bfd566830ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): RobertaForSequenceClassification(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                      (client_0): Dropout(p=0.1, inplace=False)\n",
      "                      (client_1): Dropout(p=0.1, inplace=False)\n",
      "                      (client_2): Dropout(p=0.1, inplace=False)\n",
      "                      (client_3): Dropout(p=0.1, inplace=False)\n",
      "                      (client_4): Dropout(p=0.1, inplace=False)\n",
      "                      (client_5): Dropout(p=0.1, inplace=False)\n",
      "                      (client_6): Dropout(p=0.1, inplace=False)\n",
      "                      (client_7): Dropout(p=0.1, inplace=False)\n",
      "                      (client_8): Dropout(p=0.1, inplace=False)\n",
      "                      (client_9): Dropout(p=0.1, inplace=False)\n",
      "                      (global): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_0): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_1): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_2): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_3): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_4): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_5): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_6): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_7): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_8): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_9): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (global): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_0): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_1): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_2): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_3): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_4): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_5): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_6): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_7): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_8): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_9): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (global): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                      (client_0): Dropout(p=0.1, inplace=False)\n",
      "                      (client_1): Dropout(p=0.1, inplace=False)\n",
      "                      (client_2): Dropout(p=0.1, inplace=False)\n",
      "                      (client_3): Dropout(p=0.1, inplace=False)\n",
      "                      (client_4): Dropout(p=0.1, inplace=False)\n",
      "                      (client_5): Dropout(p=0.1, inplace=False)\n",
      "                      (client_6): Dropout(p=0.1, inplace=False)\n",
      "                      (client_7): Dropout(p=0.1, inplace=False)\n",
      "                      (client_8): Dropout(p=0.1, inplace=False)\n",
      "                      (client_9): Dropout(p=0.1, inplace=False)\n",
      "                      (global): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_0): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_1): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_2): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_3): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_4): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_5): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_6): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_7): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_8): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (client_9): Linear(in_features=768, out_features=8, bias=False)\n",
      "                      (global): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_0): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_1): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_2): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_3): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_4): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_5): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_6): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_7): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_8): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (client_9): Linear(in_features=8, out_features=768, bias=False)\n",
      "                      (global): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): RobertaClassificationHead(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "        )\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_0): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_1): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_2): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_3): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_4): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_5): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_6): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_7): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_8): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (client_9): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "          (global): RobertaClassificationHead(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab8b1899-2bfa-4c6f-90bd-e9866772fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_adapters():\n",
    "    # --- collect client adapter state dicts\n",
    "    client_states = []\n",
    "    for client_id in range(USERS):\n",
    "        sd = get_peft_model_state_dict(peft_model, adapter_name=f\"client_{client_id}\")\n",
    "        if not sd:\n",
    "            raise ValueError(f\"Adapter 'client_{client_id}' not found on the model.\")\n",
    "        client_states.append(sd)\n",
    "\n",
    "    # --- current global (used for dtype/reference + to keep non-shared keys)\n",
    "    global_state = get_peft_model_state_dict(peft_model, adapter_name=\"global\")\n",
    "    if not global_state:\n",
    "        raise ValueError(\"Adapter 'global' not found on the model.\")\n",
    "\n",
    "    # --- only average keys that exist in ALL client adapters\n",
    "    common_keys = set(global_state.keys())\n",
    "    for sd in client_states:\n",
    "        common_keys &= set(sd.keys())\n",
    "    if not common_keys:\n",
    "        raise ValueError(\"No common LoRA parameter keys across client adapters to aggregate.\")\n",
    "\n",
    "    # --- average on CPU in fp32, then cast back to the global adapter's dtype\n",
    "    agg_state = {}\n",
    "    for k in common_keys:\n",
    "        stack = torch.stack([sd[k].detach().to(\"cpu\", dtype=torch.float32) for sd in client_states], dim=0)\n",
    "        avg = stack.mean(dim=0)\n",
    "        agg_state[k] = avg.to(dtype=global_state[k].dtype)\n",
    "\n",
    "    # --- keep any keys that aren't shared (e.g., modules_to_save) from global as-is\n",
    "    for k in (set(global_state.keys()) - common_keys):\n",
    "        agg_state[k] = global_state[k]\n",
    "\n",
    "    # --- write back into the 'global' adapter and ensure device matches the model\n",
    "    set_peft_model_state_dict(peft_model, agg_state, adapter_name=\"global\")\n",
    "    peft_model.to(next(peft_model.parameters()).device)\n",
    "\n",
    "    # Serialize the global adapter to disk\n",
    "    peft_model.save_pretrained(\n",
    "        \"adapters/global\",\n",
    "        selected_adapters=[\"global\"],   # save just this adapter\n",
    "        safe_serialization=True           # writes adapter_model.safetensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71366008-7113-4372-b0c4-9b96ef619066",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"prediction\":\n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"test\"][\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "def compute_perplexity(eval_model):\n",
    "    max_length = 512 # eval_model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride), desc='Batch', leave=False):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        eval_model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "    \n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "    \n",
    "        nlls.append(neg_log_likelihood)\n",
    "    \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    \n",
    "    return torch.exp(torch.stack(nlls).mean())\n",
    "\n",
    "if TASK == \"txt_classification\" or TASK == \"img_classification\":\n",
    "    metric = evaluate.load('accuracy')\n",
    "\n",
    "\n",
    "def evaluate_classification_model(inference_model, dataset):\n",
    "    eval_dataloader = DataLoader(dataset.rename_column(\"label\", \"labels\") if TASK != \"img_classification\" else dataset, batch_size=512, collate_fn=data_collator)\n",
    "\n",
    "    inference_model.to(DEVICE)\n",
    "    inference_model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc='Batch', leave=False)):\n",
    "        batch = {key: val.to(DEVICE) for key, val in batch.items() if isinstance(val, torch.Tensor)}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    return metric.compute()\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def evaluate_summarization_model(inference_model, dataset):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=256, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc='Batch', leave=False)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        reference = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        rouge.add_batch(predictions=prediction, references=reference)\n",
    "\n",
    "    rouge_score = rouge.compute()\n",
    "    print(rouge_score)\n",
    "    return rouge_score['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a29e974-9100-4e71-a8ac-9d64076d4fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Rounds:   0%|          | 0/500 [00:00<?, ?it/s]/Users/martijndevos/miniconda3/envs/delora/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training Rounds:   0%|          | 2/500 [02:42<11:13:48, 81.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TASK == \u001b[33m\"\u001b[39m\u001b[33mtxt_classification\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m TASK == \u001b[33m\"\u001b[39m\u001b[33mimg_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     inputs = {k: v.to(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m     15\u001b[39m     labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)\n\u001b[32m     16\u001b[39m     outputs = peft_model(**inputs)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for round_nr in tqdm(range(1, ROUNDS + 1), desc='Training Rounds'):\n",
    "    for user_idx in tqdm(range(USERS), desc='Users', leave=False):\n",
    "        #print(\"Training model for user %d\" % user_idx)\n",
    "        peft_model.set_adapter(\"client_%d\" % user_idx)\n",
    "        optimizer = torch.optim.AdamW(peft_model.parameters(), lr=5e-5)\n",
    "        peft_model.train()  # Set the model to training mode\n",
    "        train_dataloader = DataLoader(split_datasets[user_idx], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "        train_set_it = iter(train_dataloader)\n",
    "\n",
    "        for local_step in range(LOCAL_STEPS): # tqdm(range(LOCAL_STEPS), desc='Local Steps', leave=False):\n",
    "            batch = next(train_set_it)\n",
    "            optimizer.zero_grad()\n",
    "            if TASK == \"txt_classification\" or TASK == \"img_classification\":\n",
    "                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                outputs = peft_model(**inputs)\n",
    "                loss = cross_entropy(outputs.logits, labels)  # Calculate loss\n",
    "            elif TASK == \"prediction\" or TASK == \"summarization\":\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                masks = batch[\"attention_mask\"].to(DEVICE)\n",
    "                outputs = peft_model(input_ids=input_ids, labels=labels, attention_mask=masks)\n",
    "                loss = outputs[0]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Aggregate models\n",
    "    aggregate_adapters()\n",
    "\n",
    "    if round_nr % EVAL_INTERVAL == 0:\n",
    "        print(\"Evaluating model at round %d\" % round_nr)\n",
    "        peft_model.set_adapter(\"global\")  # Switch to the global adapter for evaluation\n",
    "        if TASK in [\"txt_classification\", \"img_classification\"]:\n",
    "            eval_res = evaluate_classification_model(peft_model, test_dataset)\n",
    "        elif TASK == \"prediction\":\n",
    "            eval_res = compute_perplexity(peft_model)\n",
    "        elif TASK == \"summarization\":\n",
    "            eval_res = evaluate_summarization_model(peft_model, test_dataset)\n",
    "        print(\"Round %d: %s\" % (round_nr, eval_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f26e49-5239-42eb-b6fd-1f7ca3158afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(peft_models[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
